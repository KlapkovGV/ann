Image classification via fine-tuning with EfficientNet

Image classification with Vision Transformer

Classification using Attention-based Deep Multiple Instance Learning

Commonly used CNN architectures: LeNet-5, AlexNet, VGG-16, ResNet, GoogLeNet

___

Training
Evaluation
CNN architectures

**Training**

Training of CNNs, like ANN training, is perfermed using gradient descent with backpropagation.

CNN function: o = f(x, w), where
- x represents the images;
- w represents the network parameters (weights, bias);
- o represents the softmax output.

During the train precess, weights that minimize the output error are calculated as E(y, f(x,w)), where y represents the target output.

Weights are changed iteratively using gradient descent.  w * (k + 1) = w(k) + eta(partial E / partial w), where
- Eta is the learning rate;
- partial E / partial w is the dradient (the partial derivative of the error with respect to the weights);
- w * (k + 1) shows how the weight for next step is updated based on the current weight w(k).

**Training – Forward Pass**

Weigth and bias values for the following network are initialized randomly (the model starts with random guesses before learning from data).

Network Architecture – multi-layer neural network

![network architecture](https://github.com/user-attachments/assets/320608ea-89ac-4c06-96bc-518d95a9767b)

- Inout layer: Consists of three nodes (i_1, i_2, i_3)
- Hidden layer 1 (h1): Uses ReLU activation function.
- Hidden layer 2 (h2): Uses the sigmoid activation function.
- Output layer: Uses the Softmax activation function, which is standard for classification tasks to produce probabilities.

Right sode of the image
- Input vector: The initial data being fed into the network is [0.1, 0.2, 0.7];
- Weighted matrices (W_{ij}, W_{jk}, W_{kl}): These matrices represent the strength of the connections between layers;
  - W_{ij} connects input to h1;
  - W_{jk} connects h1 to h2;
  - W_{kl} connects h2 to the output.
- Target output: [1.0, 0.0, 0.0] this indicates a classification problem where the first category is the "correct" answer.

**Neural Network Forward Pass - Layer 1**

In this stage, the input vector [0.1, 0.2, 0.7] is processed through the first set of weights and biases to produce the output for the hidden layer h1.

![layer h1](https://github.com/user-attachments/assets/474253bb-3a7d-41fe-9188-d743063c224b)

Matrix Operation

The input values are multiplied by the weight matrix (W_{ij}) and added to the bias vector (b) to find the raw input for the hidden layer (h1_in).

![layer h1_1](https://github.com/user-attachments/assets/c407d417-783d-4558-8553-bd42e4612e30)

The ReLU function is then applied to each value. ReLU turns any negative valu into zero and keeps positive values as they are.

Final output for layer 1: [h1_{out1}, h1_{out2}, h1_{out3}] = [1.35, 1.27, 1.8]

**Neural Network Forward Pass - Layer 2**

In this stage, the network takes the outputs generated by the first hidden layer (h1) and processes them through the second hidden layer (h2).

![layer h2](https://github.com/user-attachments/assets/da5f68d7-cc46-4a85-b55d-90caf31fad6c)

Matrix operation

THe output values from the previous layer (h1_out) act as the input for this stage. These values are multiplied by the weight matrix W_{jk} and added to the bias vector b_k to calculate the raw input for the hidden layer (h2_in).

![layer h1_2](https://github.com/user-attachments/assets/a2b31ba5-60e1-4b32-bf60-c9ef5e7176dd)

This layer applies the Sigmoid activation function to introduce non-linearity. The Sigmoid function squash values into a range between 0 and 1.

Formula: sigmoid = 1 / (1 + e^(-x))

Final output for layer 2: [h2_{out1}, h2_{out2}, h2_{out3}] = [0.938, 0.94, 0.98]

This output will be passed to the Softmax output layer to compare against the target output.

**Neural Network Forward Pass - Layer 3**

In this final stage, the network takes the outputs from the second hidden layer (h2_out) and precesses them through the output layer to produce a final prediction.

![layer h3](https://github.com/user-attachments/assets/eb21de9f-98db-40ec-837d-1530e8a6e5bc)

the output vactor from layer 2 ([0.938, 0.94, 0.98]) is multiplied by the final weight matrix W_{kl} and added to the bias vector to find the raw inputs for the final nodes (O_in).

![layer h1_3](https://github.com/user-attachments/assets/bd0d74c0-8655-4c77-a935-311c4ad4f174)

The final layer uses the Softmax function, which is standard for multi-class classification. It converts the raw scores (O_in) into probabilities that sum up to 1.0.

Formula: Softmax(x_i) = e^x_i / sum e^x_a

- Resulting probabilities (O_out): [0.2698, 0.3223, 0.4078].
- Target output: [1.0, 0.0, 0.0]

The model predicted only a -27% probability for the correct class. Because there is a large difference between 0.2698 and 1.0, the Error (Loss) will be high. For the next step the network will now use Backpropagation and Gradient Descent to go backward and adjust the weights to make this error smaller.

**Backpropagation**

The cross-entropy formula is used to measure the "distance" between these two vectors:

![backprop](https://github.com/user-attachments/assets/79ee42e2-fbcd-4e16-9388-7709b90ee4c3)

![backprop_error](https://github.com/user-attachments/assets/b94b3346-35f5-41f7-9577-f7132f26822d)

Essential functions for neural networks and their gradients used in backpropagation

![formula](https://github.com/user-attachments/assets/a0938d85-139f-4eff-b588-ce3c350f99c7)

**EfficientNet** 

Reference: Tan, M., & Le, Q. V. (2019). "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks." ICML 2019.

EfficientNet introduced a systematic approach to scaling neural networks. Before EfficientNet, scaling up CNNs was done arbitrarily. Reseachers would increase depth (more layers), width (more channels), or resolution (large images) without a principled method which led to suboptimal architectures that required manual tuning. 

EfficientNet's main contibution is compound scaling - simultaneously scaling all three dimensions (depth, width, resolution) using a simple compound coefficient with fixed ratios.

The scaling formula:

- Depth: d = α^φ
- Width: w =  β^φ
- Resolution: r = γ^φ

Subject to: α · β² · γ² ≈ 2 and α ≥ 1, β ≥ 1, γ ≥ 1

Where φ is the compound coefficient we control. The constraint ensures that for each doubling of computational resources (FLOPS), we get balanced increase acrass all dimensions.

The paper shows that balancing all three dimensions is more effective than scaling just one:
- Scaling only depth leads to vanishing gradients;
- Scaling only width captures fewer complex features;
- Scaling only resolution provides diminishing returns without more capacity.

EfficientNet Architectur

Authors started with a baseline architecture (EfficientNet-B0) found through neural architecture search (NAS), optimizing for both accuracy and FLOPS. Then they applied compound scaling to create B1-B7 variants, ach progressively larger and more accurate.

Baseline used:
- Mobile inverted bottleneck (MBConv blocks, similar to MobileNetV2);
- Squeeze-and-exciation optimization;
- Carefully designed layer configurations.

Result: EfficientNet achieved state-of-the-art accuracy on ImageNet while being significantly smaller and faster than previous models.

**EfficientNet-B7 Implementation**

```python
from tensorflow.keras.application import EfficientNetB7
from tensorflow.keras.application.efficientnet import preprocess_input, decode_predictions
import numpy as np
import tensorflow as tf

model = EfficientNetB7(weights='imagenet', include_top=True)

img = tf.keras.preprocessing.image.load_img('image.png', target_size=(600, 600))
x = tf.keras.preprocessing.image_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = model.predict(x)
print('Predicted:', decode_predictions(preds, top=3)[0])
```

The output looks like this:

```python
Predicted: [('n02504458', 'African_elephant', 0.89234), 
            ('n01871265', 'tusker', 0.05621), 
            ('n02504013', 'Indian_elephant', 0.03145)]
```

Each tuple contains:
- ImageNet class ID (e.g., 'n02504458');
- Human-readable lable (e.g., 'African_elephant');
- Confidence score (e.g., 0.89234).
